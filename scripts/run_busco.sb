#!/bin/bash --login
########## Define Resources Needed with SBATCH Lines ##########

#SBATCH --time=01:00:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --ntasks=1                 # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --cpus-per-task=8          # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem=8G                   # memory required per node - amount of memory (in bytes)
##SBATCH --array=1 #Comment out for second run
#SBATCH --array=3-9,14-22 # This is after you have validated first run
#SBATCH --job-name=run_busco       # you can give your job a name for easier identification (same as -J)
#SBATCH --output=%x-%j.SLURMout
#SBATCH --partition=scavenger

########## Command Lines to Run ##########

WORKDIR=/work/jal138/phylo/busco_out
GENOMEDIR=~/Yeast_genomes/data/genomes/
CONFIG=$GENOMEDIR/array_config.txt
mkdir -p $WORKDIR

ACC=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $2}' $CONFIG)

cd $WORKDIR
conda activate busco

busco -l microbotryomycetes_odb12 \
  -m geno --skip_bbtools \
  -c $SLURM_CPUS_PER_TASK \
  --metaeuk \
  -i $GENOMEDIR/"$ACC"_genome.fna \
  -o $ACC -f


scontrol show job $SLURM_JOB_ID     ### write job information to output file
